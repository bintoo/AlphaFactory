[
  {
    "id": "c22e7b3e",
    "generation": 1,
    "concept": "## Contamination Check (CRITICAL)\n\n1. **RSI/MACD/Moving Average mentioned?**  \n   **NO.** The paper text contains **no explicit mention** of \u201cRSI\u201d, \u201cMACD\u201d, or \u201cMoving Average\u201d. Therefore I will not suggest or use them.\n\n2. **Grid described (buying specific levels)?**  \n   **NO.** No grid/price-level ladder execution is described.  \n   **STRATEGY_TYPE:** SIGNAL (portfolio allocation / rebalancing), not grid.\n\n---\n\n## Strict Coding Plan \u2014 \u201cAlgorithm\u201d (Appendix A.1\u2013A.3) Transcribed into `OnData` Logic\nThis is an implementation plan for a quarterly-rebalanced **Hierarchical Risk Parity (HRP)** allocator with optional constraints, using only the math and steps stated in the paper.\n\n### 0) Required State (persistent variables)\nMaintain these state variables:\n\n- `lookbackDays = 252` (\u2248 trailing 12 months of daily data; paper: \u201ctrailing 12 month period\u201d)\n- `rebalanceInterval = Quarterly` (paper: \u201crebalanced at the beginning of each quarter\u201d)\n- `lastRebalanceTime` (DateTime)\n- `assets` = list of tradable symbols (current universe)\n- Rolling window storage for daily returns per asset:\n  - `returns[symbol]` = rolling window of length `lookbackDays` of daily returns\n- Constraint inputs (optional; from Appendix A.3):\n  - `assetMin[symbol] = \\bar{\u03b1}^{min}_i` and `assetMax[symbol] = \\bar{\u03b1}^{max}_i`\n  - Group mapping `groupId[symbol]`\n  - `groupMin[group] = \\bar{g}^{min}` and `groupMax[group] = \\bar{g}^{max}`\n- HRP hyperparameters:\n  - `tau` (\u03c4 in the paper; controls split rule; if you implement \u03c4-based splitting you must follow the dendrogram rule described in Section 2.3\u2014paper does not give an explicit formula beyond threshold behavior)\n  - `clusteringMethod` \u2208 {single-linkage, Ward, DIANA} OR `useGeneticPermutation` (GHRP) (Appendix A.1\u2013A.2)\n- Portfolio target:\n  - `currentTargetWeights[symbol]`\n\n---\n\n## 1) `OnData(slice)` High-Level Flow\n### Step 1.1 \u2014 Update daily returns history\nFor each `symbol` in `assets`:\n1. Compute daily return from price series (paper assumes daily total return index; implement as simple return):\n   - `r_t = P_t / P_{t-1} - 1`\n2. Push `r_t` into `returns[symbol]` rolling window.\n\n### Step 1.2 \u2014 Rebalance gate (quarterly)\nOnly proceed to weight computation if:\n- `Time` is at/after quarter start AND `Time > lastRebalanceTime` AND\n- all symbols have at least `lookbackDays` returns.\n\nIf not, exit `OnData`.\n\n---\n\n## 2) Compute Covariance and Correlation Inputs (\u03a3, \u03c1, d)\nLet `N = len(assets)`.\n\n### Step 2.1 \u2014 Build covariance matrix \u03a3 over trailing window\n- Create matrix `\u03a3` with entries:\n  - `\u03a3[i,j] = cov( returns[asset_i], returns[asset_j] )` over last `lookbackDays`.\n\n### Step 2.2 \u2014 Build correlation matrix \u03c1 from \u03a3\n- `\u03c1[i,j] = \u03a3[i,j] / (sqrt(\u03a3[i,i]) * sqrt(\u03a3[j,j]))`\n\n### Step 2.3 \u2014 Distance transform for clustering (Appendix A.1)\nThe paper defines:\n- `d_ij = sqrt( (1/2) * (1 - \u03c1_ij) )`\n\nStore `D = {d_ij}`.\n\n---\n\n## 3) Clustering + Quasi-diagonalisation (ordering)\nYou must produce an **ordered list of assets** consistent with the clustering/quasi-diagonalisation step.\n\n### Step 3.1 \u2014 Compute hierarchical clustering on D (Appendix A.1)\nDepending on configuration:\n- **Single-linkage AGNES**, **Ward**, or **DIANA** using Euclidean distance between columns of `D` (as stated).\nOutput:\n- A dendrogram / linkage structure `tree`.\n\n### Step 3.2 \u2014 Quasi-diagonalisation (Appendix A.2)\nGoal: reorder assets so \u201clargest values lie close to diagonal\u201d.\nImplementation requirement:\n- Derive an ordering `orderedAssets` from the clustering output (the \u201cordering generated by the cluster algorithm\u201d).\n\nIf using **GHRP genetic permutation** (Appendix A.2):\n- Minimise the loss:\n  - `L = \u03a3_{i,j} d_ij * (i - j)^2`, for i,j \u2208 [1..N]  (Equation 6.1 in Appendix A.2)\n- Output permutation `orderedAssets` that minimises `L`.\n- If \u03c4=1 requires an allocation tree for GHRP: derive tree by \u201crecursively searching over the range of N for the split that maximises mean absolute correlation of off-diagonal cluster blocks\u201d (Appendix A.2). (This defines how you get splits for bisection if not using dendrogram.)\n\n---\n\n## 4) Recursive Bisection Weight Computation (Appendix A.3)\nThis is the core HRP allocation logic.\n\n### Step 4.1 \u2014 Initialize containers (Appendix A.3 steps 1\u20132)\n- Start with one cluster containing all ordered assets:\n  - `L = [ orderedAssets ]`  (this is L\u2080)\n- Initialize weights:\n  - `w[symbol] = 1` for all symbols (unit weights to be rescaled)\n\n### Step 4.2 \u2014 Iterate until all clusters are singletons (Appendix A.3 step 3)\nLoop while \u2203 cluster `Li` in `L` with `|Li| > 1`:\n\nFor each such cluster `Li`:\n\n#### 4.2.a \u2014 Split cluster Li into J subsets preserving order (Appendix A.3 step 4a)\n- Split `Li` into `L_i^(1) \u222a ... \u222a L_i^(J) = Li`, order-preserving.\n- For standard HRP bisection, use J=2, but paper allows `J \u2265 2`.\n\n*(If you implement \u03c4-aware splitting: select splits consistent with the dendrogram threshold behavior described in Section 2.3; the paper does not provide a closed-form rule beyond \u03c4 extremes, so the exact split-selection mechanism must be implemented by dendrogram thresholding.)*\n\n#### 4.2.b \u2014 Compute each subset\u2019s \u201ccluster variance\u201d (Appendix A.3 step 4b)\nFor each subset `L_i^(j)`:\n\n1. Extract submatrix covariance:\n   - `V_i^j = \u03a3 restricted to assets in L_i^(j)`\n2. Compute \u201cwithin-cluster inverse-trace\u201d weights (paper\u2019s formula):\n   - `\\tilde{w}_i^j = tr[V_i^j]^{-1} / \u03a3_{k in subset} tr[V_i^j]^{-1}`\n   - Interpreting exactly as written: each asset in the subset gets proportional weight to `tr(V_i^j)^{-1}`; since `tr(V_i^j)` is scalar for the subset, this yields equal weights within subset. Implement literally:\n     - for subset size m: `\\tilde{w}_i^j` is length m with each entry = `1/m`.\n3. Cluster variance:\n   - `\\tilde{V}_i^j = (\\tilde{w}_i^j)' * V_i^j * (\\tilde{w}_i^j)`\n\n#### 4.2.c \u2014 Compute unconstrained split factors \u03b1 (Appendix A.3 step 4c)\nFor the parent cluster `Li`, compute:\n- `\u03b1_i^j = (\\tilde{V}_i^j)^{-1} / \u03a3_{j=1..J} (\\tilde{V}_i^j)^{-1}`\n\nThis produces allocation proportions across the J child subsets.\n\n#### 4.2.d \u2014 Apply constraints at this node (Appendix A.3 step 5 + group extension)\nYou must output constrained proportions `\\tilde{\u03b1}` (paper notation).\n\n**Asset-level box constraints (Appendix A.3 step 5):**\n- Given vectors `\\bar{\u03b1}^{max}` and `\\bar{\u03b1}^{min}` (length N, aligned with the assets in this parent cluster):\n  1. Clamp:\n     - `\\tilde{\u03b1} = max( min( \u03b1, \u03a3 \\bar{\u03b1}^{max} ), \u03a3 \\bar{\u03b1}^{min} )`  \n     Implement per-element clamping using the paper expression exactly as written; then\n  2. Renormalize by distributing leftover to unclamped elements (Appendix A.3 step 5b):\n     - Let `k` be indices where `\\tilde{\u03b1}_i == \u03b1_i` (not clamped)\n     - Update those:\n       - `\\tilde{\u03b1}_k = \\tilde{\u03b1}_k + |1 - \u03a3_i \\tilde{\u03b1}_i| * \u03b1_k / \u03a3_k \u03b1_k`\n  3. Repeat step 2 until `\u03a3_i \\tilde{\u03b1}_i = 1` (Appendix A.3 step 5c)\n\n**Group constraints extension (Appendix A.3, \u201cextended to facilitate group constraints\u201d):**\nBefore step 5 (clamp/renormalize), compute implied per-asset bounds:\n- For each asset i, with group g(i), let `j` denote subset of elements belonging to same group as i:\n  - `\\bar{\u03b1}^{max}_i = \u03b1_i * ( \u03a3_{j in group(i)} \\bar{g}^{max}_j ) / ( \u03a3_{j in group(i)} \u03b1_j )`\n  - `\\bar{\u03b1}^{min}_i = \u03b1_i * ( \u03a3_{j in group(i)} \u03b1_j ) / ( \u03a3_{j in group(i)} \\bar{g}^{min}_j )`\nThen run the box-constraint routine (step 5) to obtain `\\tilde{\u03b1}`.\n\n#### 4.2.e \u2014 Rescale leaf weights w by constrained \u03b1 (Appendix A.3 step 6)\nFor each child subset `L_i^(j)`:\n- Multiply all constituent asset weights `w[symbol] *= \\tilde{\u03b1}_i^j` (the factor corresponding to that subset).\n\n#### 4.2.f \u2014 Replace Li with its children in L (Appendix A.3 step 7 looping)\n- Remove `Li` from `L`\n- Add its subsets `L_i^(1..J)` to `L`\n- Continue loop until all clusters have size 1.\n\n### Step 4.3 \u2014 Final normalization\nAfter recursion, normalize across all assets:\n- `w = w / \u03a3_i w_i`  \n(Full investment constraint consistent with paper\u2019s backtest.)\n\n---\n\n## 5) Rebalance Execution (Limit-order-safe plan; no MarketOrders)\nThe paper is an allocation algorithm; it does not prescribe execution style. Your instruction forbids MarketOrders; so implement \u201cprice-level checking\u201d and limit placement.\n\n### Step 5.1 \u2014 Compute target holdings\n- `targetValue[symbol] = Portfolio.TotalPortfolioValue * w[symbol]`\n- `targetQty[symbol] = targetValue[symbol] / currentPrice`\n\n### Step 5.2 \u2014 Place/adjust LimitOrders only\nFor each symbol:\n1. `deltaQty = targetQty - currentHoldingsQty`\n2. If `abs(deltaQty)` below a tolerance, skip.\n3. If `deltaQty > 0` (need to buy):\n   - Place a **LimitOrder** at `limitPrice = currentPrice` (or slightly below if you must define a rule; the paper provides no rule\u2014so keep it at current price to remain compliant with \u201cONLY the math found in the paper\u201d).\n4. If `deltaQty < 0` (need to sell):\n   - Place a **LimitOrder** at `limitPrice = currentPrice` (or slightly above; again not specified\u2014keep at current price).\n\nMaintain outstanding orders and cancel/replace as needed on the next `OnData` if still open.\n\n### Step 5.3 \u2014 Update `lastRebalanceTime = Time`\nStore `currentTargetWeights = w`.\n\n---\n\n## 6) Summary of Exact Math Used (paper-only)\n- Correlation-distance transform: `d_ij = sqrt( (1/2) * (1 - \u03c1_ij) )`\n- Diagonalisation loss for GHRP: `L = \u03a3_{i,j} d_ij (i - j)^2`\n- Cluster variance: `\\tilde{V} = \\tilde{w}' V \\tilde{w}`\n- Split weights: `\u03b1_j = ( \\tilde{V}_j )^{-1} / \u03a3_j ( \\tilde{V}_j )^{-1}`\n- Constraint enforcement: clamp to min/max and renormalize via:\n  - `\\tilde{\u03b1}_k = \\tilde{\u03b1}_k + |1 - \u03a3 \\tilde{\u03b1}| * \u03b1_k / \u03a3_k \u03b1_k`, iterate until sum=1\n- Quarterly rebalance; trailing 12-month covariance estimation.\n\nIf you want, I can convert this plan into pseudocode structured exactly like an `OnData` method (with helper functions for clustering, ordering, and recursive bisection), still keeping strictly to the paper\u2019s math.",
    "code": "from AlgorithmImports import *\nimport numpy as np\nfrom collections import deque\n\nclass MyAgentStrategy(QCAlgorithm):\n    def Initialize(self):\n        self.SetStartDate(2020, 1, 1)\n        self.SetEndDate(2025, 1, 1)\n        self.SetCash(100000)\n        self.SetWarmUp(timedelta(days=30))\n\n        self.lookbackDays = 252\n        self.lastRebalanceTime = datetime.min\n\n        self.symbols = []\n        tickers = ['SPY', 'QQQ', 'IWM', 'EFA', 'EEM', 'TLT', 'IEF', 'GLD', 'VNQ', 'XLE']\n        for ticker in tickers:\n            equity = self.AddEquity(ticker, Resolution.Daily)\n            self.symbols.append(equity.Symbol)\n\n        self.returns = {s: deque(maxlen=self.lookbackDays) for s in self.symbols}\n        self.prevClose = {s: None for s in self.symbols}\n\n        self.assetMin = {s: 0.0 for s in self.symbols}\n        self.assetMax = {s: 1.0 for s in self.symbols}\n\n        self.groupId = {s: str(s) for s in self.symbols}\n        self.groupMin = {self.groupId[s]: 0.0 for s in self.symbols}\n        self.groupMax = {self.groupId[s]: 1.0 for s in self.symbols}\n\n        self.currentTargetWeights = {s: 0.0 for s in self.symbols}\n\n        self.openOrderIdsBySymbol = {s: [] for s in self.symbols}\n\n        self.RecoverState()\n\n    def RecoverState(self):\n        history = self.History(self.symbols, self.lookbackDays + 1, Resolution.Daily)\n        if history is None or history.empty:\n            return\n\n        for s in self.symbols:\n            try:\n                df = history.loc[s]\n            except Exception:\n                continue\n\n            if df is None or df.empty:\n                continue\n            if 'close' not in df.columns:\n                continue\n\n            closes = df['close'].dropna()\n            if len(closes) < 2:\n                continue\n\n            self.returns[s].clear()\n            prev = None\n            for _, c in closes.items():\n                if prev is None:\n                    prev = float(c)\n                    continue\n                c = float(c)\n                if prev != 0:\n                    r = c / prev - 1.0\n                    self.returns[s].append(float(r))\n                prev = c\n\n            self.prevClose[s] = float(closes.iloc[-1])\n\n    def OnData(self, data: Slice):\n        for s in self.symbols:\n            if not data.Bars.ContainsKey(s):\n                continue\n            bar = data.Bars[s]\n            close = float(bar.Close)\n            prev = self.prevClose[s]\n            if prev is not None and prev != 0:\n                r = close / prev - 1.0\n                self.returns[s].append(float(r))\n            self.prevClose[s] = close\n\n        self.ManageOpenOrders()\n\n        if self.IsWarmingUp:\n            return\n\n        if not self.IsQuarterStart(self.Time):\n            return\n        if self.Time <= self.lastRebalanceTime:\n            return\n        if not self.AllReadyForCovariance():\n            return\n\n        assets = list(self.symbols)\n        ret_matrix = self.BuildReturnMatrix(assets)\n        if ret_matrix is None:\n            return\n\n        cov = np.cov(ret_matrix, rowvar=False, ddof=1)\n        if cov is None or cov.shape[0] != len(assets):\n            return\n\n        corr = self.CovToCorr(cov)\n        dist = np.sqrt(0.5 * (1.0 - corr))\n\n        ordered_assets = self.SingleLinkageOrder(assets, dist)\n        if ordered_assets is None or len(ordered_assets) != len(assets):\n            return\n\n        weights = self.HRPWeights(ordered_assets, cov, assets)\n        if weights is None:\n            return\n\n        ssum = sum(weights.values())\n        if ssum <= 0:\n            return\n        for s in weights:\n            weights[s] = float(weights[s] / ssum)\n\n        self.currentTargetWeights = dict(weights)\n\n        self.RebalanceWithLimitOrders(weights)\n\n        self.lastRebalanceTime = self.Time\n\n    def CancelOrder(self, orderId: int):\n        try:\n            self.Transactions.CancelOrder(orderId)\n        except Exception:\n            pass\n\n    def ManageOpenOrders(self):\n        for s in self.symbols:\n            ids = self.openOrderIdsBySymbol.get(s, [])\n            if not ids:\n                continue\n            for oid in list(ids):\n                self.CancelOrder(oid)\n            self.openOrderIdsBySymbol[s] = []\n\n    def IsQuarterStart(self, time: datetime) -> bool:\n        is_first_day = time.day == 1\n        is_quarter_month = time.month in [1, 4, 7, 10]\n        return is_first_day and is_quarter_month\n\n    def AllReadyForCovariance(self) -> bool:\n        for s in self.symbols:\n            if len(self.returns[s]) < self.lookbackDays:\n                return False\n        return True\n\n    def BuildReturnMatrix(self, assets):\n        T = self.lookbackDays\n        N = len(assets)\n        mat = np.zeros((T, N), dtype=float)\n        for j, s in enumerate(assets):\n            r = list(self.returns[s])\n            if len(r) < T:\n                return None\n            mat[:, j] = np.array(r[-T:], dtype=float)\n        return mat\n\n    def CovToCorr(self, cov: np.ndarray) -> np.ndarray:\n        n = cov.shape[0]\n        corr = np.zeros((n, n), dtype=float)\n        diag = np.diag(cov)\n        std = np.sqrt(np.maximum(diag, 1e-18))\n        for i in range(n):\n            for j in range(n):\n                denom = std[i] * std[j]\n                if denom > 0:\n                    corr[i, j] = cov[i, j] / denom\n                else:\n                    corr[i, j] = 0.0\n        corr = np.clip(corr, -1.0, 1.0)\n        return corr\n\n    def SingleLinkageOrder(self, assets, dist):\n        n = len(assets)\n        if n <= 1:\n            return list(assets)\n\n        clusters = {i: [i] for i in range(n)}\n        active = set(clusters.keys())\n        next_id = n\n        merges = {}\n\n        def cluster_distance(a_id, b_id):\n            a = clusters[a_id]\n            b = clusters[b_id]\n            m = float('inf')\n            for i in a:\n                for j in b:\n                    d = float(dist[i, j])\n                    if d < m:\n                        m = d\n            return m\n\n        while len(active) > 1:\n            active_list = list(active)\n            best = float('inf')\n            best_pair = None\n            for i in range(len(active_list)):\n                for j in range(i + 1, len(active_list)):\n                    ai = active_list[i]\n                    bj = active_list[j]\n                    d = cluster_distance(ai, bj)\n                    if d < best:\n                        best = d\n                        best_pair = (ai, bj)\n\n            if best_pair is None:\n                return None\n\n            left, right = best_pair\n            clusters[next_id] = clusters[left] + clusters[right]\n            merges[next_id] = (left, right)\n\n            active.remove(left)\n            active.remove(right)\n            active.add(next_id)\n            next_id += 1\n\n        root = list(active)[0]\n\n        def leaf_order(node_id):\n            if node_id < n:\n                return [node_id]\n            left, right = merges[node_id]\n            return leaf_order(left) + leaf_order(right)\n\n        idx_order = leaf_order(root)\n        return [assets[i] for i in idx_order]\n\n    def HRPWeights(self, ordered_assets, cov_full, full_assets):\n        index = {s: i for i, s in enumerate(full_assets)}\n\n        L = [list(ordered_assets)]\n        w = {s: 1.0 for s in ordered_assets}\n\n        while True:\n            big = [c for c in L if len(c) > 1]\n            if len(big) == 0:\n                break\n\n            new_L = []\n            for cluster in L:\n                if len(cluster) == 1:\n                    new_L.append(cluster)\n                    continue\n\n                m = len(cluster)\n                split = m // 2\n                left = cluster[:split]\n                right = cluster[split:]\n                children = [left, right]\n\n                variances = []\n                for subset in children:\n                    V = self.SubCov(cov_full, subset, index)\n                    if V is None:\n                        return None\n                    v = self.ClusterVarianceIVP(V)\n                    v = max(float(v), 1e-18)\n                    variances.append(float(v))\n\n                inv_vars = [1.0 / v for v in variances]\n                inv_sum = sum(inv_vars)\n                if inv_sum <= 0:\n                    return None\n                alpha = [iv / inv_sum for iv in inv_vars]\n\n                constrained_alpha = self.ConstrainSplit(cluster, children, alpha)\n                if constrained_alpha is None:\n                    return None\n\n                for j, subset in enumerate(children):\n                    a = float(constrained_alpha[j])\n                    for s in subset:\n                        w[s] *= a\n\n                new_L.extend(children)\n\n            L = new_L\n\n        total = sum(w.values())\n        if total <= 0:\n            return None\n        for s in w:\n            w[s] = float(w[s] / total)\n        return w\n\n    def ClusterVarianceIVP(self, V: np.ndarray) -> float:\n        diag = np.diag(V)\n        diag = np.maximum(diag, 1e-18)\n        iv = 1.0 / diag\n        s = float(np.sum(iv))\n        if s <= 0:\n            m = len(diag)\n            ww = np.ones((m, 1), dtype=float) / float(m)\n            v = float((ww.T @ V @ ww)[0, 0])\n            return float(v)\n        ww = (iv / s).reshape(-1, 1)\n        v = float((ww.T @ V @ ww)[0, 0])\n        return float(v)\n\n    def SubCov(self, cov_full, subset, index_map):\n        idx = [index_map[s] for s in subset]\n        V = cov_full[np.ix_(idx, idx)]\n        return V\n\n    def ConstrainSplit(self, parent_cluster, children, alpha):\n        asset_alpha = {}\n        for j, subset in enumerate(children):\n            if len(subset) == 0:\n                return None\n            per_asset = float(alpha[j]) / float(len(subset))\n            for s in subset:\n                asset_alpha[s] = per_asset\n\n        group_assets = {}\n        for s in parent_cluster:\n            g = self.groupId[s]\n            if g not in group_assets:\n                group_assets[g] = []\n            group_assets[g].append(s)\n\n        asset_max = {}\n        asset_min = {}\n        for s in parent_cluster:\n            g = self.groupId[s]\n            ga = group_assets[g]\n\n            sum_alpha_group = sum(float(asset_alpha[x]) for x in ga)\n            gmax = float(self.groupMax.get(g, 1.0))\n            gmin = float(self.groupMin.get(g, 0.0))\n            ai = float(asset_alpha[s])\n\n            if sum_alpha_group <= 0:\n                asset_max[s] = float(self.assetMax.get(s, 1.0))\n                asset_min[s] = float(self.assetMin.get(s, 0.0))\n                continue\n\n            # Treat groupMax/groupMin as TOTAL group caps (do not multiply by group size)\n            amax_i = ai * (gmax / sum_alpha_group) if gmax >= 0 else float(self.assetMax.get(s, 1.0))\n\n            if gmin > 0:\n                amin_i = ai * (sum_alpha_group / gmin)\n            else:\n                amin_i = float(self.assetMin.get(s, 0.0))\n\n            asset_max[s] = min(float(self.assetMax.get(s, 1.0)), float(amax_i))\n            asset_min[s] = max(float(self.assetMin.get(s, 0.0)), float(amin_i))\n\n        subset_min = []\n        subset_max = []\n        for subset in children:\n            subset_min.append(sum(float(asset_min[s]) for s in subset))\n            subset_max.append(sum(float(asset_max[s]) for s in subset))\n\n        constrained = self.ClampAndRenormalize(alpha, subset_min, subset_max)\n        return constrained\n\n    def ClampAndRenormalize(self, alpha, amin, amax):\n        J = len(alpha)\n        a = [float(x) for x in alpha]\n\n        t = []\n        for j in range(J):\n            lo = float(amin[j])\n            hi = float(amax[j])\n            x = float(a[j])\n            if x > hi:\n                x = hi\n            if x < lo:\n                x = lo\n            t.append(x)\n\n        for _ in range(50):\n            ssum = sum(t)\n            diff = 1.0 - ssum\n            if abs(diff) < 1e-10:\n                break\n\n            k = []\n            for j in range(J):\n                if abs(t[j] - a[j]) < 1e-12:\n                    k.append(j)\n\n            if len(k) == 0:\n                if ssum > 0:\n                    t = [x / ssum for x in t]\n                break\n\n            denom = sum(a[j] for j in k)\n            if denom <= 0:\n                break\n\n            for j in k:\n                t[j] = t[j] + abs(diff) * a[j] / denom\n\n            for j in range(J):\n                lo = float(amin[j])\n                hi = float(amax[j])\n                if t[j] > hi:\n                    t[j] = hi\n                if t[j] < lo:\n                    t[j] = lo\n\n        ssum = sum(t)\n        if ssum > 0:\n            t = [x / ssum for x in t]\n        return t\n\n    def RebalanceWithLimitOrders(self, weights):\n        total_value = float(self.Portfolio.TotalPortfolioValue)\n\n        for s in self.symbols:\n            if s not in weights:\n                continue\n            if not self.Securities.ContainsKey(s):\n                continue\n\n            price = float(self.Securities[s].Price)\n            if price <= 0:\n                continue\n\n            target_value = total_value * float(weights[s])\n            target_qty = target_value / price\n\n            current_qty = float(self.Portfolio[s].Quantity)\n            delta_qty = target_qty - current_qty\n\n            if abs(delta_qty) < 1.0:\n                continue\n\n            for oid in list(self.openOrderIdsBySymbol.get(s, [])):\n                self.CancelOrder(oid)\n            self.openOrderIdsBySymbol[s] = []\n\n            qty = int(delta_qty)\n            if qty == 0:\n                continue\n\n            order_id = self.LimitOrder(s, qty, price)\n            self.openOrderIdsBySymbol[s].append(order_id)",
    "metrics": {},
    "parents": []
  }
]